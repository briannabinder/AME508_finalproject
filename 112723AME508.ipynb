{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/briannabinder/AME508_finalproject/blob/main/112723AME508.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vxBUCyHGxYMU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import base64\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "from torchsummary import summary\n",
        "from skimage.transform import resize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piyoc1CTxdcH"
      },
      "source": [
        "Drive mount from my personal driver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yBo35vNZxeFf"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWDy3Jd30clV"
      },
      "source": [
        "Wav to spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urxRHXY079Db",
        "outputId": "93c2c5ab-581f-4c53-d989-6a523e4a1f9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Masaomi Enami\\AppData\\Local\\Temp\\ipykernel_20528\\2676354469.py:23: RuntimeWarning: divide by zero encountered in log10\n",
            "  log_spectrogram = 10 * np.log10(spectrogram)\n"
          ]
        }
      ],
      "source": [
        "# Specify the folder path\n",
        "#folder_path = '/content/drive/MyDrive/Colab_Notebooks/Project_Datasets/Train_Datasets'\n",
        "folder_path = 'G:\\My Drive\\Colab_Notebooks\\Project_Datasets\\Train_Datasets'\n",
        "\n",
        "# Get a list of all files in the folder\n",
        "file_list = [f for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
        "\n",
        "# List to store all log_spectrograms\n",
        "all_spectrograms = []\n",
        "\n",
        "# Iterate through each .wav file in the folder\n",
        "for wav_file in file_list:\n",
        "    # Construct the full path to the .wav file\n",
        "    wav_file_path = os.path.join(folder_path, wav_file)\n",
        "\n",
        "    # Read the content of the .wav file\n",
        "    sample_rate, samples = wavfile.read(wav_file_path)\n",
        "\n",
        "    # Compute the spectrogram\n",
        "    frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
        "\n",
        "    # Apply logarithmic transformation\n",
        "    log_spectrogram = 10 * np.log10(spectrogram)\n",
        "\n",
        "    # Store the log_spectrogram in the list\n",
        "    all_spectrograms.append(log_spectrogram)\n",
        "\n",
        "# Print the first data point for the first file\n",
        "print(f\"File: {file_list[0]}, Frequency: {frequencies[0]} Hz, Time: {times[0]} sec\")\n",
        "\n",
        "# Plot the log-scaled spectrogram for the first file\n",
        "plt.pcolormesh(times, frequencies, all_spectrograms[0], shading='auto')\n",
        "plt.ylabel('Frequency [Hz]')\n",
        "plt.xlabel('Time [sec]')\n",
        "plt.title(f'Log-Scaled Spectrogram - {file_list[0]}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_651V5N2KmQM"
      },
      "source": [
        "ReSize 129x267 Reshape to VGG16 size of 224 × 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k4BeYvqhsxr"
      },
      "outputs": [],
      "source": [
        "# Specify the target size\n",
        "target_size = (224, 224)\n",
        "\n",
        "# List to store all resized log_spectrograms\n",
        "all_resized_spectrograms = []\n",
        "\n",
        "# Iterate through each log_spectrogram\n",
        "for log_spectrogram in all_spectrograms:\n",
        "    # Resize the log_spectrogram to the target size\n",
        "    resized_log_spectrogram = resize(log_spectrogram, target_size, anti_aliasing=True)\n",
        "\n",
        "    # Store the resized log_spectrogram in the list\n",
        "    all_resized_spectrograms.append(resized_log_spectrogram)\n",
        "\n",
        "\n",
        "all_resized_spectrograms = np.reshape(np.array(all_resized_spectrograms), newshape=(7809,1,224,224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z3V9UoEhhaj"
      },
      "source": [
        "Test datasets .wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B16vkX32hcnJ"
      },
      "outputs": [],
      "source": [
        "# Specify the folder path\n",
        "#folder_path1 = '/content/drive/MyDrive/Colab_Notebooks/Project_Datasets/Test_Datasets'\n",
        "folder_path1 ='G:\\My Drive\\Colab_Notebooks\\Project_Datasets\\Test_Datasets'\n",
        "\n",
        "# Get a list of all files in the folder\n",
        "file_list = [f for f in os.listdir(folder_path1) if f.endswith('.wav')]\n",
        "\n",
        "# List to store all log_spectrograms\n",
        "all_spectrograms = []\n",
        "\n",
        "# Iterate through each .wav file in the folder\n",
        "for wav_file in file_list:\n",
        "    # Construct the full path to the .wav file\n",
        "    wav_file_path = os.path.join(folder_path1, wav_file)\n",
        "\n",
        "    # Read the content of the .wav file\n",
        "    sample_rate, samples = wavfile.read(wav_file_path)\n",
        "\n",
        "    # Compute the spectrogram\n",
        "    frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
        "\n",
        "    # Apply logarithmic transformation\n",
        "    log_spectrogram = 10 * np.log10(spectrogram)\n",
        "\n",
        "    # Store the log_spectrogram in the list\n",
        "    all_spectrograms.append(log_spectrogram)\n",
        "\n",
        "# Specify the target size\n",
        "target_size = (224, 224)\n",
        "\n",
        "# List to store all resized log_spectrograms\n",
        "all_resized_spectrograms1 = []\n",
        "\n",
        "# Iterate through each log_spectrogram\n",
        "for log_spectrogram in all_spectrograms:\n",
        "    # Resize the log_spectrogram to the target size\n",
        "    resized_log_spectrogram = resize(log_spectrogram, target_size, anti_aliasing=True)\n",
        "\n",
        "    # Store the resized log_spectrogram in the list\n",
        "    all_resized_spectrograms1.append(resized_log_spectrogram)\n",
        "\n",
        "all_resized_spectrograms1 = np.reshape(np.array(all_resized_spectrograms1), newshape=(1956,1,224,224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IARsbyHB0hs3"
      },
      "source": [
        "Train datasets Label Reader Mulit Hot Incoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqzOmaKzV68X"
      },
      "outputs": [],
      "source": [
        "# Get a list of all files in the folder\n",
        "file_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "# Specify the labels to encode\n",
        "labels_to_encode = ['Wheeze', 'Stridor', 'Rhonchi', 'Crackles']\n",
        "\n",
        "# List to store all multi-hot encoded matrices\n",
        "all_multi_hot_matrices = []\n",
        "\n",
        "# Iterate through each file\n",
        "for file_name in file_list:\n",
        "    if file_name.endswith('.txt'):  # Check if the file is a .txt file\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Read the content of the .txt file with different encodings\n",
        "        for encoding in ['utf-8', 'latin-1', 'ISO-8859-1']:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding=encoding) as file:\n",
        "                    label_lines = file.readlines()\n",
        "                break  # Stop trying encodings if successful\n",
        "            except UnicodeDecodeError:\n",
        "                continue  # Try the next encoding if decoding fails\n",
        "\n",
        "        # Parse labels and time intervals\n",
        "        labels_and_intervals = []\n",
        "        for line in label_lines:\n",
        "            parts = line.strip().split()\n",
        "            if parts and parts[0] != 'D':  # Check if the line is not empty and doesn't start with 'D'\n",
        "                label = parts[0]\n",
        "                labels_and_intervals.append(label)\n",
        "\n",
        "        # Create a single-row multi-hot encoded matrix for the current file\n",
        "        multi_hot_encoded_vector = []\n",
        "\n",
        "        for label_row in labels_to_encode:\n",
        "            # Check if the label_row is in the labels_and_intervals\n",
        "            if label_row in labels_and_intervals:\n",
        "                multi_hot_vector = 1\n",
        "            else:\n",
        "                multi_hot_vector = 0\n",
        "\n",
        "            multi_hot_encoded_vector.append(multi_hot_vector)\n",
        "\n",
        "        # Append the multi-hot encoded vector to the list\n",
        "        all_multi_hot_matrices.append({\n",
        "            'file_name': file_name,\n",
        "            'vector': multi_hot_encoded_vector\n",
        "        })\n",
        "\n",
        "# Create a NumPy array to store all the vectors\n",
        "multi_hot_matrices = np.array([item['vector'] for item in all_multi_hot_matrices])\n",
        "\n",
        "# Iterate through each dictionary in all_multi_hot_matrices\n",
        "for item in all_multi_hot_matrices:\n",
        "    # Access file_name and vector using the keys\n",
        "    file_name = item['file_name']\n",
        "    vector = item['vector']\n",
        "\n",
        "    # Print or use the values as needed\n",
        "    #print(f\"File: {file_name}\")\n",
        "    #print(labels_to_encode)\n",
        "    #print(vector)\n",
        "    #print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
        "\n",
        "# Print the total count of files\n",
        "print(f\"Total count of files: {len(all_multi_hot_matrices)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkeI7Gvj-X--"
      },
      "source": [
        "Test_datasets Label Reader Mulit Hot Incoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfMT798Q-ykc"
      },
      "outputs": [],
      "source": [
        "# Get a list of all files in the folder\n",
        "file_list = [f for f in os.listdir(folder_path1) if os.path.isfile(os.path.join(folder_path1, f))]\n",
        "\n",
        "# Specify the labels to encode\n",
        "labels_to_encode = ['Wheeze', 'Stridor', 'Rhonchi', 'Crackles']\n",
        "\n",
        "# List to store all multi-hot encoded matrices\n",
        "all_multi_hot_matrices1 = []\n",
        "\n",
        "# Iterate through each file\n",
        "for file_name in file_list:\n",
        "    if file_name.endswith('.txt'):  # Check if the file is a .txt file\n",
        "        file_path = os.path.join(folder_path1, file_name)\n",
        "\n",
        "        # Read the content of the .txt file with different encodings\n",
        "        for encoding in ['utf-8', 'latin-1', 'ISO-8859-1']:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding=encoding) as file:\n",
        "                    label_lines = file.readlines()\n",
        "                break  # Stop trying encodings if successful\n",
        "            except UnicodeDecodeError:\n",
        "                continue  # Try the next encoding if decoding fails\n",
        "\n",
        "        # Parse labels and time intervals\n",
        "        labels_and_intervals = []\n",
        "        for line in label_lines:\n",
        "            parts = line.strip().split()\n",
        "            if parts and parts[0] != 'D':  # Check if the line is not empty and doesn't start with 'D'\n",
        "                label = parts[0]\n",
        "                labels_and_intervals.append(label)\n",
        "\n",
        "        # Create a single-row multi-hot encoded matrix for the current file\n",
        "        multi_hot_encoded_vector1 = []\n",
        "\n",
        "        for label_row in labels_to_encode:\n",
        "            # Check if the label_row is in the labels_and_intervals\n",
        "            if label_row in labels_and_intervals:\n",
        "                multi_hot_vector = 1\n",
        "            else:\n",
        "                multi_hot_vector = 0\n",
        "\n",
        "            multi_hot_encoded_vector1.append(multi_hot_vector)\n",
        "\n",
        "        # Append the multi-hot encoded vector to the list\n",
        "        all_multi_hot_matrices1.append({\n",
        "            'file_name': file_name,\n",
        "            'vector': multi_hot_encoded_vector1\n",
        "        })\n",
        "\n",
        "# Create a NumPy array to store all the vectors\n",
        "multi_hot_matrices1 = np.array([item['vector'] for item in all_multi_hot_matrices1])\n",
        "\n",
        "# Iterate through each dictionary in all_multi_hot_matrices\n",
        "for item in all_multi_hot_matrices1:\n",
        "    # Access file_name and vector using the keys\n",
        "    file_name = item['file_name']\n",
        "    vector = item['vector']\n",
        "\n",
        "    # Print or use the values as needed\n",
        "    #print(f\"File: {file_name}\")\n",
        "    #print(labels_to_encode)\n",
        "    #print(vector)\n",
        "    #print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
        "\n",
        "# Print the total count of files\n",
        "print(f\"Total count of files: {len(all_multi_hot_matrices1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3rUBso9WtJO"
      },
      "source": [
        "CustomDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr4FPXwqFXWr"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, samples, labels):\n",
        "        \"\"\"\n",
        "        Initialize the CustomDataset with paired samples.\n",
        "\n",
        "        Args:\n",
        "            samples (list of tuples): A list of (x, y) pairs representing the dataset samples.\n",
        "        \"\"\"\n",
        "        self.samples = torch.Tensor(samples).to(torch.float32)\n",
        "        self.labels = torch.Tensor(labels).to(torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset, i.e., the number of samples.\n",
        "        \"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns the sample pairs corresponding to the given list of indices.\n",
        "\n",
        "        Args:\n",
        "            indices (list): A list of indices to retrieve samples for.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of (x, y) pairs corresponding to the specified indices.\n",
        "        \"\"\"\n",
        "        selected_sample = self.samples[idx]\n",
        "        selected_label = self.labels[idx]\n",
        "        return selected_sample , selected_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah8wAiCTFyx_"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "train_dataset = CustomDataset(all_resized_spectrograms,multi_hot_matrices)\n",
        "\n",
        "test_dataset = CustomDataset(all_resized_spectrograms1,multi_hot_matrices1)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size , shuffle=True)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=1956)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBmnMfihRXyn"
      },
      "source": [
        " VGG16 CNN as feature extractors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA9cB28oRa39"
      },
      "outputs": [],
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, verbose=False):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(25088, 4096)\n",
        "        self.dropout = nn.Dropout(0.5)  # Add dropout with a probability of 0.5\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.dropout = nn.Dropout(0.5)  # Add dropout with a probability of 0.5\n",
        "        self.fc3 = nn.Linear(4096, 4)\n",
        "\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.verbose:\n",
        "            print(f\"Input shape: {x.size()}\")\n",
        "\n",
        "        x = F.relu(self.conv1_1(x))\n",
        "        x = F.relu(self.conv1_2(x))\n",
        "        x = self.pool1(x)\n",
        "        if self.verbose:\n",
        "            print(f\"After Layer 1: {x.size()}\")\n",
        "\n",
        "        x = F.relu(self.conv2_1(x))\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        x = self.pool2(x)\n",
        "        if self.verbose:\n",
        "            print(f\"After Layer 2: {x.size()}\")\n",
        "\n",
        "        x = F.relu(self.conv3_1(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        x = F.relu(self.conv3_3(x))\n",
        "        x = self.pool3(x)\n",
        "        if self.verbose:\n",
        "            print(f\"After Layer 3: {x.size()}\")\n",
        "\n",
        "        x = F.relu(self.conv4_1(x))\n",
        "        x = F.relu(self.conv4_2(x))\n",
        "        x = F.relu(self.conv4_3(x))\n",
        "        x = self.pool4(x)\n",
        "        if self.verbose:\n",
        "            print(f\"After Layer 4: {x.size()}\")\n",
        "\n",
        "        x = F.relu(self.conv5_1(x))\n",
        "        x = F.relu(self.conv5_2(x))\n",
        "        x = F.relu(self.conv5_3(x))\n",
        "        x = self.pool5(x)\n",
        "        if self.verbose:\n",
        "            print(f\"After Layer 5 {x.size()}\")\n",
        "\n",
        "        x = torch.flatten(x,1)\n",
        "        #x = torch.unsqueeze(1,x)\n",
        "        if self.verbose:\n",
        "            print(f\"Flattened: {x.size()}\")\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        if self.verbose:\n",
        "            print(f\"After first Fully connected layer: {x.size()}\")\n",
        "\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        if self.verbose:\n",
        "            print(f\"After dropout: {x.size()}\")\n",
        "\n",
        "        x = F.relu(self.fc2(x))\n",
        "        if self.verbose:\n",
        "            print(f\"After Secound Fully connected layer: {x.size()}\")\n",
        "\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        if self.verbose:\n",
        "            print(f\"After dropout: {x.size()}\")\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        if self.verbose:\n",
        "            print(f\"Output shape: {x.size()}\")\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlRGCijhU8dI"
      },
      "source": [
        "Check for GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC_7q2-JX_Wi"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNCnrAd1VBFZ"
      },
      "source": [
        "Testing VGG16 CNN shape size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASUiZ3KM1x1p"
      },
      "outputs": [],
      "source": [
        "print(\"Intermediate shapes for VGG16\")\n",
        "model = VGG16(verbose=True)\n",
        "output = model(torch.ones(size=(1,1,224,224)))\n",
        "# More infor on VGG16 (https://builtin.com/machine-learning/vgg16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjtAvH5da3Da"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgjYPpCpX7Mx"
      },
      "source": [
        "Training VGG16 CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ5GViyvX6ay"
      },
      "outputs": [],
      "source": [
        "model = VGG16().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),weight_decay=1e-5)\n",
        "n_epoch   = 1\n",
        "n_batches = len(trainloader)\n",
        "print_every = 10\n",
        "\n",
        "loss_hist = np.zeros(n_epoch*n_batches)\n",
        "acc_hist  = np.zeros(n_epoch*n_batches)\n",
        "\n",
        "it = 0\n",
        "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
        "\n",
        "    loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # save loss\n",
        "        loss_hist[it] = loss.item()\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        acc_hist[it] = 100.0*np.sum(predicted.cpu().numpy() == np.argmax(labels.numpy(), axis=1))/labels.size(0)\n",
        "\n",
        "        if (i+1) % print_every == 0:    # print every 50 mini-batches\n",
        "            #vacc_hist[iter//print_every] = find_vacc(net,testloader)\n",
        "            print('epoch = {}/{}, minibatch = {}/{}, loss = {}, accuracy = {}'.format(epoch+1, n_epoch, i+1, n_batches, loss.item(), acc_hist[it]))\n",
        "\n",
        "        it+=1\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-IiGHwFYZWB"
      },
      "source": [
        "Plotting training losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0goY4E69YZA0"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 1, figsize=(15, 10))\n",
        "axs[0].plot(loss_hist, label = \"TrainDatasets\")\n",
        "axs[0].set_yscale('log')\n",
        "axs[0].set_ylabel('Training Loss')\n",
        "axs[0].legend()\n",
        "\n",
        "\n",
        "axs[1].plot(acc_hist, label = \"TrainDatasets\")\n",
        "axs[1].set_ylabel('Accuracy [%]')\n",
        "axs[1].set_xlabel('Iterations')\n",
        "axs[1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTWX4o5hIGpO"
      },
      "source": [
        "Save the variable of training model for reusing purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ygFOT7wIPFr"
      },
      "outputs": [],
      "source": [
        "# Save all relevant information at the end of training\n",
        "#Store_variable = {\n",
        " #   'model_state_dict': model.state_dict(),\n",
        " #   'optimizer_state_dict': optimizer.state_dict(),\n",
        " #   'loss_history': loss_hist,\n",
        " #   'accuracy_history': acc_hist,\n",
        " #   'train_dataset' : train_dataset,\n",
        " #   'test_dataset' : test_dataset,\n",
        " #   'trainloader' : trainloader,\n",
        " #   'testloader' : testloader,\n",
        "#}\n",
        "\n",
        "# Save the model to a file\n",
        "torch.save({model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss_history': loss_hist,\n",
        "    'accuracy_history': acc_hist,\n",
        "    'train_dataset' : train_dataset,\n",
        "    'test_dataset' : test_dataset,\n",
        "    'trainloader' : trainloader,\n",
        "    'testloader' : testloader}, )\n",
        "# Create a download link link from Jupyter Notebook\n",
        "from IPython.display import FileLink\n",
        "FileLink(Store_variable)\n",
        "\n",
        "# Load the model from the file\n",
        "model, optimizer, loss_history, accuracy_history, train_dataset,test_dataset,trainloader,testloader = torch.load(Store_variable).values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CphZClPx8KdN"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4CSwRlI8UxC"
      },
      "outputs": [],
      "source": [
        "# Find accuracy over tasting set\n",
        "\n",
        "def find_vacc(net,loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels = data\n",
        "            # calculate outputs by running images through the network\n",
        "            outputs = net(images.to(device))\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += np.sum(predicted.cpu().numpy() == np.argmax(labels.numpy(), axis=1))\n",
        "    vacc = 100.0 * correct / total\n",
        "    return vacc\n",
        "\n",
        "test_acc = find_vacc(model, testloader)\n",
        "\n",
        "print(f'Accuracy of the network trained on the test images: {test_acc} %')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNHjQYwWpswmkhwU+bzNhne",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
